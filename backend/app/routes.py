import os
import random
import uuid
import re
import unicodedata
from flask import Blueprint, request, jsonify
from firebase_admin import firestore 
from openai import OpenAI

# --- OpenAI ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
AI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
client = OpenAI(api_key=OPENAI_API_KEY)

# --- Flask BP ---
main = Blueprint('main', __name__)

# Perguntas curtas (fallback; n√£o usamos no kickoff contextual)
perguntas_aleatorias = [
    "Voc√™ prefere trabalhar com tecnologia ou com pessoas?",
    "Voc√™ se v√™ em um trabalho criativo ou mais t√©cnico?",
    "Voc√™ gosta de resolver problemas ou prefere realizar tarefas repetitivas?",
    "Voc√™ se imagina em um cargo de lideran√ßa ou prefere executar tarefas espec√≠ficas?",
    "Voc√™ gostaria de trabalhar em um ambiente din√¢mico ou em um local mais tranquilo?",
    "Voc√™ prefere trabalhar sozinho ou em equipe?",
    "Voc√™ gostaria de trabalhar em uma √°rea mais voltada para vendas ou mais t√©cnica?",
]

GREETINGS = {
    "oi", "ola", "ol√°", "bom dia", "boa tarde", "boa noite",
    "hey", "hello", "hi", "iniciar", "come√ßar", "start", "comecar",
    "opa", "eai", "e a√≠"
}

# Texto institucional para a FAQ "O que √© o Vocational Test?"
ABOUT_VT_TEXT = (
    "O Vocational Test √© uma plataforma que usa IA para ajudar jovens a descobrirem √°reas e carreiras "
    "mais compat√≠veis com seus interesses, valores e objetivos. Funciona em duas frentes: "
    "(1) um **chat vocacional inteligente** que conduz perguntas curtas e contextuais para entender seu perfil; "
    "e (2) **an√°lises estruturadas** com recomenda√ß√µes de √°reas e carreiras, caminhos de forma√ß√£o e pr√≥ximos passos. "
    "Voc√™ conversa, a IA aprende com suas respostas e gera um resumo final com indica√ß√µes pr√°ticas. "
    "Quer come√ßar me contando uma atividade que voc√™ gosta de fazer no dia a dia?"
)

def gerar_pergunta_aleatoria():
    return random.choice(perguntas_aleatorias)

# Contagem por chat para encerramento autom√°tico
chats_contagem_perguntas = {}

def gerar_chat_id():
    return str(uuid.uuid4())

# ---------- Persist√™ncia ----------
def salvar_mensagem(chat_id: str, conteudo: str, tipo: str = "resposta"):
    db = firestore.client()
    mensagens_ref = db.collection('chats').document(chat_id).collection('mensagens')
    mensagens_ref.add({
        'tipo': tipo,  # 'pergunta' | 'resposta' | 'resumo_final'
        'conteudo': conteudo,
        'timestamp': firestore.SERVER_TIMESTAMP
    })

def salvar_resposta_firebase(chat_id: str, pergunta: str, resposta: str, etapa: str | None = None):
    db = firestore.client()
    respostas_ref = db.collection('chats').document(chat_id).collection('respostas')
    payload = {
        'pergunta': pergunta,
        'resposta': resposta,
        'timestamp': firestore.SERVER_TIMESTAMP
    }
    if etapa:
        payload['etapa'] = etapa
    respostas_ref.add(payload)
    salvar_mensagem(chat_id, pergunta, tipo="pergunta")
    salvar_mensagem(chat_id, resposta, tipo="resposta")

def salvar_resultado_analise(chat_id: str, analise_texto: str, fonte: str = "analise-perfil"):
    db = firestore.client()
    resultados_ref = db.collection('chats').document(chat_id).collection('resultados')
    resultados_ref.add({
        'fonte': fonte,
        'resultado': analise_texto,
        'timestamp': firestore.SERVER_TIMESTAMP
    })
    salvar_mensagem(chat_id, analise_texto, tipo="resumo_final" if "resumo" in (fonte or "") else "resposta")

# ---------- Utilidades ----------
def _strip_accents(s: str) -> str:
    if not s:
        return ""
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

def normalizar_texto(s: str) -> str:
    s = (s or "").strip().lower()
    s = _strip_accents(s)
    s = re.sub(r"\s+", " ", s)
    return s

def revisar_resposta(resposta: str) -> str:
    # Mant√©m respostas objetivas e bem pontuadas, sem cortar o conte√∫do do modelo.
    txt = (resposta or "").strip()
    if len(txt.split()) < 5:
        return "Desculpe, n√£o consegui entender sua resposta completamente. Pode reformular em 1 ou 2 frases?"
    if not txt.endswith(('.', '!', '?')):
        txt += '.'
    # Redirecionamento gentil se a pessoa disser que n√£o sabe
    if 'n√£o sei' in txt.lower() or 'nao sei' in txt.lower() or 'indeciso' in txt.lower():
        txt = "Sem problemas! Qual atividade do seu dia a dia voc√™ curte fazer e que te d√° energia?"
    return txt

def montar_contexto_do_chat(chat_id: str) -> str:
    db = firestore.client()
    respostas_ref = db.collection('chats').document(chat_id).collection('respostas')
    snaps = respostas_ref.order_by('timestamp').stream()

    blocos = []
    for doc in snaps:
        d = doc.to_dict() or {}
        pergunta = d.get('pergunta', '')
        resposta = d.get('resposta', '')
        etapa = d.get('etapa')
        if etapa:
            blocos.append(f"[{etapa}] Q: {pergunta}\nA: {resposta}")
        else:
            blocos.append(f"Q: {pergunta}\nA: {resposta}")

    return "\n\n".join(blocos) if blocos else "Sem hist√≥rico suficiente."

def montar_contexto_compacto(chat_id: str, max_pares: int = 8, max_chars: int = 3000) -> str:
    """
    Vers√£o compacta do contexto: pega apenas os √∫ltimos N pares Q/A de 'respostas',
    preservando a ordem cronol√≥gica e limitando o tamanho total.
    """
    db = firestore.client()
    respostas_ref = db.collection('chats').document(chat_id).collection('respostas')

    snaps = list(respostas_ref.order_by('timestamp', direction=firestore.Query.DESCENDING).limit(max_pares).stream())
    pares = []
    for doc in reversed(snaps):  # volta √† ordem cronol√≥gica
        d = doc.to_dict() or {}
        pergunta = (d.get('pergunta') or '').strip()
        resposta = (d.get('resposta') or '').strip()
        etapa = d.get('etapa')
        if etapa:
            pares.append(f"[{etapa}] Q: {pergunta}\nA: {resposta}")
        else:
            pares.append(f"Q: {pergunta}\nA: {resposta}")

    contexto = "\n\n".join(pares).strip() or "Sem hist√≥rico suficiente."
    if len(contexto) > max_chars:
        contexto = contexto[-max_chars:]  # mant√©m o final (as intera√ß√µes mais recentes)
    return contexto

def is_greeting(texto: str) -> bool:
    t = normalizar_texto(texto)
    return t in GREETINGS

def gerar_frase_saudacao(texto: str) -> str:
    t = normalizar_texto(texto)
    if "bom dia" in t:
        return "Bom dia! üòä"
    if "boa tarde" in t:
        return "Boa tarde! üòä"
    if "boa noite" in t:
        return "Boa noite! üòä"
    if "hello" in t or "hi" in t or "hey" in t:
        return "Hello! üëã"
    if "opa" in t or "eai" in t or "e a√≠" in t:
        return "Opa! üòÑ"
    return "Ol√°! üòä"

# ---------- Intentos/FAQs ----------
FAQ_TRIGGERS = [
    r"\bo que e\b",
    r"\bsobre\b",
    r"\bcomo funciona\b",
    r"\bquem sao voces\b",
    r"\bquem e voces\b"
]
VT_TRIGGERS = [
    r"\bvocational test\b",
    r"\bvt\b"
]

def eh_pergunta_sobre_vt(texto: str) -> bool:
    t = normalizar_texto(texto)
    return any(re.search(p, t) for p in FAQ_TRIGGERS) and any(re.search(v, t) for v in VT_TRIGGERS)

# ---------- Endpoints ----------
@main.route('/api/chat-vocacional', methods=['POST'])
def chat_vocacional():
    data = request.get_json() or {}
    pergunta = data.get('mensagem', '') or ''
    chat_id = data.get('chat_id', gerar_chat_id())

    ENCERRAR = 'encerrar teste' in normalizar_texto(pergunta)
    contagem_atual = chats_contagem_perguntas.get(chat_id, 0)
    kickoff = (contagem_atual == 0) and (pergunta.strip() == '' or is_greeting(pergunta))

    try:
        # Encerramento expl√≠cito
        if ENCERRAR:
            contexto = montar_contexto_do_chat(chat_id)
            msgs = [
                {"role": "system", "content": "Voc√™ √© um orientador vocacional. Resuma de forma clara e objetiva, em PT-BR."},
                {"role": "user", "content": f"Com base no hist√≥rico abaixo, gere um RESUMO FINAL curto (6‚Äì10 linhas) com: perfil, √°reas aderentes e pr√≥ximos passos.\n\n{contexto}"}
            ]
            response = client.chat.completions.create(
                model=AI_MODEL,
                messages=msgs,
                max_tokens=300,
                temperature=0.6
            )
            resumo = response.choices[0].message.content.strip()
            salvar_resultado_analise(chat_id, resumo, fonte="resumo-final-chat")
            return jsonify({"resposta": resumo, "finalizado": True, "pergunta_aleatoria": None})

        # FAQ: "o que √©/sobre/como funciona + vocational test/vt"
        if eh_pergunta_sobre_vt(pergunta):
            salvar_resposta_firebase(chat_id, pergunta, ABOUT_VT_TEXT)
            # N√£o incrementa contagem para n√£o consumir intera√ß√£o do usu√°rio
            return jsonify({"resposta": ABOUT_VT_TEXT, "pergunta_aleatoria": None})

        # --- IN√çCIO DA CONVERSA: se √© sauda√ß√£o, responder com OI e depois fazer a pergunta contextual ---
        if kickoff:
            contexto = montar_contexto_do_chat(chat_id)
            # Geramos apenas a PERGUNTA (sem cumprimentos) via modelo
            msgs = [
                {"role": "system", "content":
                    "Voc√™ √© um orientador vocacional. Gere UMA √∫nica pergunta curta e espec√≠fica em PT-BR, "
                    "sem cumprimentos, baseada no hist√≥rico do question√°rio do usu√°rio. "
                    "Foque em h√°bitos do dia a dia relacionados aos interesses citados. "
                    "Apenas 1 pergunta, terminando com '?'."},
                {"role": "user", "content": f"Hist√≥rico (perguntas e respostas):\n\n{contexto}\n\nGere UMA pergunta contextual √∫nica."}
            ]
            response = client.chat.completions.create(
                model=AI_MODEL,
                messages=msgs,
                max_tokens=60,
                temperature=0.6
            )
            pergunta_contextual = (response.choices[0].message.content or "").strip()

            # Monta a sauda√ß√£o apropriada + a pergunta, numa mesma mensagem (1 bolha)
            saudacao = gerar_frase_saudacao(pergunta)
            resposta_final = f"{saudacao} {pergunta_contextual}"

            # N√£o incrementamos contagem aqui (ainda n√£o houve resposta do usu√°rio)
            salvar_mensagem(chat_id, resposta_final, tipo="resposta")
            return jsonify({"resposta": resposta_final, "pergunta_aleatoria": None})

        # --- Fluxo normal: agora COM CONTEXTO compacto e regras de ader√™ncia ---
        contexto_compacto = montar_contexto_compacto(chat_id, max_pares=8, max_chars=3000)

        system_prompt = (
            "Voc√™ √© um assistente vocacional objetivo e educado. Responda em PT-BR com 1‚Äì3 frases claras, "
            "SEM cumprimentos. Primeiro, espelhe 2‚Äì3 palavras do que a pessoa acabou de dizer (para mostrar entendimento), "
            "em seguida avance com a ideia principal. Se a nova mensagem estiver fora do tema do contexto, "
            "redirecione gentilmente para interesses, valores, objetivos ou limita√ß√µes j√° citados. "
            "Evite repetir perguntas j√° feitas. Sempre finalize com UMA pergunta curta e espec√≠fica para avan√ßar a conversa."
        )

        user_prompt = (
            f"Contexto (√∫ltimas intera√ß√µes de pergunta e resposta):\n{contexto_compacto}\n\n"
            f"Nova mensagem do usu√°rio: {pergunta}\n\n"
            "Regra: mantenha a conversa no mesmo assunto, conectando explicitamente a resposta ao contexto acima."
        )

        mensagens_completas = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        response = client.chat.completions.create(
            model=AI_MODEL,
            messages=mensagens_completas,
            max_tokens=180,
            temperature=0.7
        )

        conteudo = (response.choices[0].message.content or "").strip()
        conteudo = revisar_resposta(conteudo)

        # Persist√™ncia + contagem
        chats_contagem_perguntas[chat_id] = contagem_atual + 1
        salvar_resposta_firebase(chat_id, pergunta, conteudo)

        # Encerramento autom√°tico a partir da 10¬™ intera√ß√£o
        if chats_contagem_perguntas[chat_id] >= 10:
            contexto = montar_contexto_do_chat(chat_id)
            msgs = [
                {"role": "system", "content": "Voc√™ √© um orientador vocacional. Resuma de forma clara e objetiva, em PT-BR."},
                {"role": "user", "content": f"Gere um RESUMO FINAL (6‚Äì10 linhas) com perfil, √°reas indicadas e pr√≥ximos passos, com base no hist√≥rico:\n\n{contexto}"}
            ]
            r2 = client.chat.completions.create(
                model=AI_MODEL,
                messages=msgs,
                max_tokens=320,
                temperature=0.6
            )
            resumo = (r2.choices[0].message.content or "").strip()
            salvar_resultado_analise(chat_id, resumo, fonte="resumo-final-automatico")
            return jsonify({"resposta": resumo, "finalizado": True, "pergunta_aleatoria": None})

        return jsonify({"resposta": conteudo, "pergunta_aleatoria": None})

    except Exception as e:
        print(f"Erro na IA (chat-vocacional): {e}")
        return jsonify({
            "resposta": "Desculpe, n√£o consegui processar sua resposta agora. Tente novamente em alguns instantes."
        }), 500

@main.route('/api/analise-perfil', methods=['POST'])
def analise_perfil():
    """
    Entrada: { "chat_id": "...", "respostas": [ { "etapa": "...", "pergunta": "...", "resposta": "..." }, ... ] }
    Sa√≠da:   { "analise": "..." }
    """
    data = request.get_json() or {}
    chat_id = data.get('chat_id', gerar_chat_id())
    respostas = data.get('respostas', [])

    if not respostas:
        return jsonify({"analise": "N√£o recebemos respostas do question√°rio."}), 400

    try:
        blocos = []
        for item in respostas:
            etapa = item.get('etapa', '')
            pergunta = item.get('pergunta', '')
            resposta = item.get('resposta', '')
            blocos.append(f"[{etapa}]\nPergunta: {pergunta}\nResposta: {resposta}")
            salvar_resposta_firebase(chat_id, pergunta, resposta, etapa=etapa)

        contexto = "\n\n".join(blocos)

        system_msg = (
            "Voc√™ √© um orientador vocacional profissional. "
            "Baseado no question√°rio a seguir, produza uma an√°lise clara, em PT-BR, com esta estrutura:"
            "\n1) S√≠ntese do perfil"
            "\n2) Perfis Holland prov√°veis + justificativa"
            "\n3) √Åreas de atua√ß√£o compat√≠veis (humanas, exatas, biol√≥gicas, t√©cnicas)"
            "\n4) 6‚Äì10 carreiras recomendadas (bullets, com justificativa curta)"
            "\n5) Caminhos de forma√ß√£o e primeiros passos pr√°ticos"
            "\nRegras: objetivo, sem repetir perguntas, sem frases cortadas."
        )

        user_msg = f"Respostas do question√°rio:\n\n{contexto}\n\nGere a an√°lise seguindo exatamente a estrutura."

        response = client.chat.completions.create(
            model=AI_MODEL,
            messages=[{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}],
            max_tokens=900,
            temperature=0.6
        )

        analise = (response.choices[0].message.content or "").strip()
        salvar_resultado_analise(chat_id, analise, fonte="analise-perfil")

        return jsonify({"analise": analise})

    except Exception as e:
        print(f"Erro na IA (analise-perfil): {e}")
        return jsonify({"analise": "N√£o foi poss√≠vel gerar a an√°lise agora. Tente novamente em alguns minutos."}), 500

@main.route("/healthz", methods=["GET"])
def health_check():
    return "OK", 200
